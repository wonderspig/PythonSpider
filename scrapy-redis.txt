参考信息：http://blog.csdn.net/hjhmpl123/article/details/53292602
	http://blog.csdn.net/u013378306/article/details/54017933


Master只有一个redis数据库，主要负责url的去重，以及任务的调度。


scrapy-redis原理: 
1.spider解析下载器下载下来的response,返回item或者是links 
2.item或者links经过spidermiddleware的process_spider_out()方法，交给engine。 
3.engine将item交给itempipeline,将links交给调度器 
4.在调度器中，先将request对象利用scrapy内置的指纹函数，生成一个指纹对象 
5.如果request对象中的dont_filter参数设置为False,并且该request对象的指纹不在信息指纹的队列中，那么就把该request对象放到优先级的队列中 
6.从优先级队列中获取request对象，交给engine 
7.engine将request对象交给下载器下载，期间会通过downloadmiddleware的process_request()方法 
8.下载器完成下载，获得response对象，将该对象交给engine,期间会通过downloadmiddleware的process_response()方法 
9.engine将获得的response对象交给spider进行解析，期间会经过spidermiddleware的process_spider_input()方法 
10.从第一步开始循环。
上面的十个步骤就是scrapy-redis的整体框架，与scrapy相差无几。本质的区别就是，将scrapy的内置的去重的队列和待抓取的request队列换成了redis的集合。就这一个小小的改动，就使得了scrapy-redis支持了分布式抓取。

在redis的服务器中，会至少存在三个队列： 
a.用于请求对象去重的集合，队列的名称为spider.name:dupefilter，其中spider.name就是我们自定义的spider的名字，下同。 
b.待抓取的request对象的有序集合，队列的名称为spider.name:requests 
c.保存提取到item的列表，队列的名称为spider.name:items 
d.可能存在存放初始url的集合或者是列表，队列的名称可能是spider.name:start_urls



可能存在的问题：
1、一定时间内单个IP访问次数，一个正常用户访问网站，除非是随意的点着玩，否则不会在一段持续时间内过快访问一个网站，持续时间也不会太长。这个问题好办，我们可以采用大量不规则代理IP形成一个代理池，随机从代理池中选择代理，模拟访问。代理IP有两种，透明代理和匿名代理。

2、一定时间内单个账号访问次数，如果一个人一天24小时都在访问一个数据接口，而且速度非常快，那就有可能是机器人了。我们可以采用大量行为正常的账号，行为正常就是普通人怎么在社交网站上操作，并且单位时间内，访问URL数目尽量减少，可以在每次访问中间间隔一段时间，这个时间间隔可以是一个随机值，即每次访问完一个URL，随机随眠一段时间，再接着访问下一个URL。
如果能把账号和IP的访问策略控制好了，基本就没什么问题了。当然对方网站也会有运维会调整策略，敌我双方的一场较量，爬虫必须要能感知到对方的反监控将会对我们有影响，通知管理员及时处理。其实最理想的是能够通过机器学习，智能的实现反监控对抗，实现不间断地抓取

关于spider,crawler区别：
spider、crawler、indexer。一开始是spider根据URI，访问进来，接着，读取服务器的header和网页的head标签。然后，crawler顺着spider发现的网页的内链，去访问该内链的另一端。最后，indexer来读取HTML代码。


